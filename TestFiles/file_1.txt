Creating a plagiarism detector involves comparing two text documents to determine the similarity between them. There are several algorithms and methods commonly used for detecting plagiarism. Here are some of the top algorithms you can consider implementing:

1. **Cosine Similarity**:
   - This method calculates the cosine of the angle between two vectors. These vectors represent text documents in a multi-dimensional space, with each dimension corresponding to a word from the documents. The cosine similarity ranges from -1 (completely different) to 1 (exactly the same), with higher values indicating more similarity.

2. **Jaccard Similarity**:
   - The Jaccard similarity measures the similarity between two sets. It is defined as the size of the intersection divided by the size of the union of the sample sets. For text comparison, the sets are the sets of words (or n-grams) in each document.

3. **Levenshtein Distance (Edit Distance)**:
   - This algorithm measures how many single-character edits (insertions, deletions, or substitutions) are necessary to change one word into another. It can be adapted to compare larger text blocks by considering how many edits are needed to transform one document into another.

4. **Rabin-Karp Algorithm**:
   - Primarily used for string searching, the Rabin-Karp algorithm uses hashing to find any one set of pattern strings in a text. It is effective in plagiarism detection for finding matching sequences of text within larger documents.

5. **Winnowing Algorithm**:
   - This algorithm is used to detect plagiarism by creating fingerprints of documents. It involves selecting a subset of the hash values of all the k-grams of a document. The selected fingerprints are then compared to determine the similarity between documents, making it efficient for large-scale comparisons.

6. **Longest Common Subsequence**:
   - This method finds the longest subsequence present in both texts, which doesn't have to be contiguous. It's useful for measuring the similarity between documents where some parts are rearranged.

7. **Bag of Words**:
   - This is a simplifying representation used in natural language processing and information retrieval. In this model, a text (such as a sentence or a document) is represented as an unordered collection of words, disregarding grammar and even word order but keeping multiplicity.

8. **TF-IDF (Term Frequency-Inverse Document Frequency)**:
   - This statistical measure evaluates how relevant a word is to a document in a collection of documents. It is often used in search engine scoring, text summarization, and document clustering.

9. **Machine Learning Approaches**:
   - More advanced plagiarism detectors use machine learning algorithms such as support vector machines (SVM), neural networks, or decision trees to classify documents based on their features, which can include textual similarity measures among others.

Each algorithm has its strengths and is suited to different aspects of plagiarism detection, such as detecting exact copies or paraphrasing. For effective plagiarism detection, you might consider combining several methods or adapting them based on the specifics of your needs, like the nature of the texts and the level of detail needed in the analysis.